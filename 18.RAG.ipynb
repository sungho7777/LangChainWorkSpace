{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0178bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .env 파일에 저장된 환경변수들을 파이썬 환경으로 불러오는 역할\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb476cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain 프레임워크에서 OpenAI의 ChatOpenAI 모델을 사용하는 예\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec5b623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace에서 제공하는 임베딩 모델 \"BAAI/bge-m3\"를 Langchain을 통해 사용하는 코드\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "model_name = \"BAAI/bge-m3\"\n",
    "hf_embeddings = HuggingFaceEmbeddings(model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f36b334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 52, which is longer than the specified 0\n",
      "Created a chunk of size 46, which is longer than the specified 0\n",
      "Created a chunk of size 43, which is longer than the specified 0\n",
      "Created a chunk of size 47, which is longer than the specified 0\n",
      "Created a chunk of size 41, which is longer than the specified 0\n",
      "Created a chunk of size 41, which is longer than the specified 0\n",
      "Created a chunk of size 47, which is longer than the specified 0\n",
      "Created a chunk of size 38, which is longer than the specified 0\n",
      "Created a chunk of size 40, which is longer than the specified 0\n",
      "Created a chunk of size 41, which is longer than the specified 0\n",
      "Created a chunk of size 38, which is longer than the specified 0\n",
      "Created a chunk of size 46, which is longer than the specified 0\n",
      "Created a chunk of size 36, which is longer than the specified 0\n",
      "Created a chunk of size 35, which is longer than the specified 0\n",
      "Created a chunk of size 45, which is longer than the specified 0\n",
      "Created a chunk of size 39, which is longer than the specified 0\n",
      "Created a chunk of size 47, which is longer than the specified 0\n",
      "Created a chunk of size 47, which is longer than the specified 0\n",
      "Created a chunk of size 43, which is longer than the specified 0\n"
     ]
    }
   ],
   "source": [
    "# Langchain을 사용하여 텍스트 파일을 로드하고, 문서를 청크(조각) 단위로 분리하는 작업\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=0,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "loader = TextLoader(\"./docs/travel.txt\", encoding=\"utf-8\")\n",
    "documents = loader.load_and_split(text_splitter=text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a00ebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# 문서(documents)를 Chroma 벡터 DB에 저장하기 위한 Langchain 코드\n",
    "from langchain_chroma import Chroma\n",
    "db = Chroma.from_documents(documents, hf_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da402667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chroma 벡터 DB에서 MMR(Maximal Marginal Relevance) 방식을 사용하는 Retriever 객체를 생성\n",
    "# MMR은 “서로 비슷하지 않으면서도 질문과 관련성 높은 결과” 를 고르는 알고리즘\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5b4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain에서 사용할 **프롬프트 템플릿(ChatPromptTemplate)**을 정의하는 예시로, \n",
    "# RAG (Retrieval-Augmented Generation) 흐름에서 매우 일반적으로 사용되는 형태\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "아래 제공된 내용만 참고하여 사용자의 질문에 답변해줘. 모르면 솔직하게 그냥 모른다고 답해.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "User Input:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", message)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88014b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\langsmith\\client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x0000016E6A28BCE0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LangChain의 Hub 기능을 이용하여 미리 정의된 고품질 프롬프트를 가져오는 예시\n",
    "from langchain import hub\n",
    "# https://smith.langchain.com/hub/langchain-ai/retrieval-qa-chat\n",
    "prompt_template = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa781cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StrOutputParser는 Langchain에서 출력 파서(Output Parser) 역할을 하는 클래스\n",
    "# LLM이 생성한 응답을 문자열(str) 형태로 바로 반환하는 가장 단순한 파서\n",
    "# 별도의 복잡한 파싱(예: JSON 파싱, 키-값 추출 등)이 필요 없고, 단순 텍스트를 그대로 받으려 할 때 사용\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b6d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langchain에서 RAG(검색 증강 생성) 파이프라인을 Runnable 체인으로 구성한 예\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "rag_chain = {\n",
    "    \"context\": retriever,\n",
    "    \"input\": RunnablePassthrough()\n",
    " } | prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fabb5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'이색적인 숙소로는 트리하우스, 동굴 호텔, 빙하 호텔 등이 있습니다. 이러한 독특한 숙소 체험이 가능합니다.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"이색적인 숙소에는 어떤 곳이 있을까?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "503f8ae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'주어진 문서 내용에는 우주의 역사에 대한 정보가 포함되어 있지 않습니다. 그러나 일반적으로 우주의 역사는 빅뱅으로 시작하여, 우주가 팽창하고 별과 은하가 형성되며, 생명이 존재하는 행성이 나타나는 과정을 포함합니다. 더 구체적인 내용을 알고 싶다면 다른 출처를 참고하시기를 권장합니다.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"우주의 역사에 대해서 알려줘\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
